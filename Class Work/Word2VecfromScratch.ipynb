{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yi4byzXtUpv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a1786de-48cb-4c76-fbb7-6f92cce960f0"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PFquMQptZG5"
      },
      "source": [
        "def tokenize(text):\n",
        "    # obtains tokens with a least 1 alphabet\n",
        "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
        "    return pattern.findall(text.lower())\n",
        "\n",
        "def mapping(tokens):\n",
        "    word_to_id = dict()\n",
        "    id_to_word = dict()\n",
        "\n",
        "    for i, token in enumerate(set(tokens)):\n",
        "        word_to_id[token] = i\n",
        "        id_to_word[i] = token\n",
        "\n",
        "    return word_to_id, id_to_word\n",
        "\n",
        "def generate_training_data(tokens, word_to_id, window_size):\n",
        "    N = len(tokens)\n",
        "    X, Y = [], []\n",
        "\n",
        "    for i in range(N):\n",
        "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
        "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
        "        #print(nbr_inds)\n",
        "        #print(tokens[i])\n",
        "        for j in nbr_inds:\n",
        "            X.append(word_to_id[tokens[i]])\n",
        "            #print('X:',word_to_id[tokens[i]])\n",
        "            #print(tokens[j])\n",
        "            Y.append(word_to_id[tokens[j]])\n",
        "            #print('Y:',word_to_id[tokens[j]])\n",
        "            \n",
        "    X = np.array(X)\n",
        "    X = np.expand_dims(X, axis=0)\n",
        "    Y = np.array(Y)\n",
        "    Y = np.expand_dims(Y, axis=0)\n",
        "            \n",
        "    return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKMcaDgzt6o7",
        "outputId": "0c6d0dc1-b31a-42a6-f307-c4687cf0194d"
      },
      "source": [
        "doc = \"After the deduction of the costs of investing, \" \\\n",
        "      \"beating the stock market is a loser's game.\"\n",
        "tokens = tokenize(doc)\n",
        "print(tokens)\n",
        "word_to_id, id_to_word = mapping(tokens)\n",
        "print(word_to_id)\n",
        "print(id_to_word)\n",
        "X, Y = generate_training_data(tokens, word_to_id, 3)\n",
        "print(X)\n",
        "print(Y)\n",
        "vocab_size = len(id_to_word)\n",
        "print(vocab_size)\n",
        "m = Y.shape[1]\n",
        "print(m)\n",
        "# turn Y into one hot encoding\n",
        "Y_one_hot = np.zeros((vocab_size, m))\n",
        "Y_one_hot[Y.flatten(), np.arange(m)] = 1\n",
        "print(Y_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['after', 'the', 'deduction', 'of', 'the', 'costs', 'of', 'investing', 'beating', 'deduction', 'the', 'stock', 'market', 'deduction', 'is', 'a', \"loser's\", 'game']\n",
            "{\"loser's\": 0, 'costs': 1, 'stock': 2, 'is': 3, 'after': 4, 'beating': 5, 'deduction': 6, 'of': 7, 'game': 8, 'the': 9, 'investing': 10, 'market': 11, 'a': 12}\n",
            "{0: \"loser's\", 1: 'costs', 2: 'stock', 3: 'is', 4: 'after', 5: 'beating', 6: 'deduction', 7: 'of', 8: 'game', 9: 'the', 10: 'investing', 11: 'market', 12: 'a'}\n",
            "[[ 4  4  4  9  9  9  9  6  6  6  6  6  7  7  7  7  7  7  9  9  9  9  9  9\n",
            "   1  1  1  1  1  1  7  7  7  7  7  7 10 10 10 10 10 10  5  5  5  5  5  5\n",
            "   6  6  6  6  6  6  9  9  9  9  9  9  2  2  2  2  2  2 11 11 11 11 11 11\n",
            "   6  6  6  6  6  6  3  3  3  3  3  3 12 12 12 12 12  0  0  0  0  8  8  8]]\n",
            "[[ 9  6  7  4  6  7  9  4  9  7  9  1  4  9  6  9  1  7  9  6  7  1  7 10\n",
            "   6  7  9  7 10  5  7  9  1 10  5  6  9  1  7  5  6  9  1  7 10  6  9  2\n",
            "   7 10  5  9  2 11 10  5  6  2 11  6  5  6  9 11  6  3  6  9  2  6  3 12\n",
            "   9  2 11  3 12  0  2 11  6 12  0  8 11  6  3  0  8  6  3 12  8  3 12  0]]\n",
            "13\n",
            "96\n",
            "[[0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y.flatten()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSuWqWbzWEXq",
        "outputId": "c7bb2cf8-286c-4829-e369-147f7624a5e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7,  4,  8, 11,  4,  8,  7, 11,  7,  8,  7, 10, 11,  7,  4,  7, 10,\n",
              "        8,  7,  4,  8, 10,  8, 12,  4,  8,  7,  8, 12,  9,  8,  7, 10, 12,\n",
              "        9,  7,  7, 10,  8,  9,  7,  5, 10,  8, 12,  7,  5,  6,  8, 12,  9,\n",
              "        5,  6,  3, 12,  9,  7,  6,  3,  1,  9,  7,  5,  3,  1,  0,  7,  5,\n",
              "        6,  1,  0,  2,  5,  6,  3,  0,  2,  6,  3,  1,  2,  3,  1,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.arange(84)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxEEV4JGWX1Y",
        "outputId": "9e866206-d301-4081-f2cd-1f57357dee1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
              "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
              "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPaZTo8nuAcD",
        "outputId": "bd38ec02-c995-4431-ec2b-554e218f739b"
      },
      "source": [
        "Y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 84)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Raid0GnKwteX"
      },
      "source": [
        "def initialize_wrd_emb(vocab_size, emb_size):\n",
        "    \"\"\"\n",
        "    vocab_size: int. vocabulary size of your corpus or training data\n",
        "    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n",
        "    \"\"\"\n",
        "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
        "    \n",
        "    # assert(WRD_EMB.shape == (vocab_size, emb_size))\n",
        "    return WRD_EMB\n",
        "\n",
        "def initialize_dense(input_size, output_size):\n",
        "    \"\"\"\n",
        "    input_size: int. size of the input to the dense layer\n",
        "    output_szie: int. size of the output out of the dense layer\n",
        "    \"\"\"\n",
        "    W = np.random.randn(output_size, input_size) * 0.01\n",
        "    \n",
        "    #assert(W.shape == (output_size, input_size))\n",
        "    return W\n",
        "\n",
        "def initialize_parameters(vocab_size, emb_size):\n",
        "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
        "    W = initialize_dense(emb_size, vocab_size)\n",
        "    \n",
        "    parameters = {}\n",
        "    parameters['WRD_EMB'] = WRD_EMB\n",
        "    parameters['W'] = W\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufAFalZoze4H"
      },
      "source": [
        "def ind_to_word_vecs(inds, parameters):\n",
        "    \"\"\"\n",
        "    inds: numpy array. shape: (1, m)\n",
        "    parameters: dict. weights to be trained\n",
        "    \"\"\"\n",
        "    m = inds.shape[1]\n",
        "    WRD_EMB = parameters['WRD_EMB']\n",
        "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
        "    \n",
        "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
        "    \n",
        "    return word_vec\n",
        "\n",
        "def linear_dense(word_vec, parameters):\n",
        "    \"\"\"\n",
        "    word_vec: numpy array. shape: (emb_size, m)\n",
        "    parameters: dict. weights to be trained\n",
        "    \"\"\"\n",
        "    m = word_vec.shape[1]\n",
        "    W = parameters['W']\n",
        "    Z = np.dot(W, word_vec)\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], m))\n",
        "    \n",
        "    return W, Z\n",
        "\n",
        "def softmax(Z):\n",
        "    \"\"\"\n",
        "    Z: output out of the dense layer. shape: (vocab_size, m)\n",
        "    \"\"\"\n",
        "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
        "    \n",
        "    assert(softmax_out.shape == Z.shape)\n",
        "    return softmax_out\n",
        "\n",
        "def forward_propagation(inds, parameters):\n",
        "    word_vec = ind_to_word_vecs(inds, parameters)\n",
        "    W, Z = linear_dense(word_vec, parameters)\n",
        "    softmax_out = softmax(Z)\n",
        "    \n",
        "    caches = {}\n",
        "    caches['inds'] = inds\n",
        "    caches['word_vec'] = word_vec\n",
        "    caches['W'] = W\n",
        "    caches['Z'] = Z\n",
        "    \n",
        "    return softmax_out, caches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuGsS8hPzjUx"
      },
      "source": [
        "def cross_entropy(softmax_out, Y):\n",
        "    \"\"\"\n",
        "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
        "    \"\"\"\n",
        "    m = softmax_out.shape[1]\n",
        "    cost = -(1 / m) * np.sum(np.sum(Y * np.log(softmax_out + 0.001), axis=0, keepdims=True), axis=1)\n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNHj1ajzzp7A"
      },
      "source": [
        "def softmax_backward(Y, softmax_out):\n",
        "    \"\"\"\n",
        "    Y: labels of training data. shape: (vocab_size, m)\n",
        "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
        "    \"\"\"\n",
        "    dL_dZ = softmax_out - Y\n",
        "    \n",
        "    assert(dL_dZ.shape == softmax_out.shape)\n",
        "    return dL_dZ\n",
        "\n",
        "def dense_backward(dL_dZ, caches):\n",
        "    \"\"\"\n",
        "    dL_dZ: shape: (vocab_size, m)\n",
        "    caches: dict. results from each steps of forward propagation\n",
        "    \"\"\"\n",
        "    W = caches['W']\n",
        "    word_vec = caches['word_vec']\n",
        "    m = word_vec.shape[1]\n",
        "    \n",
        "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
        "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
        "\n",
        "    assert(W.shape == dL_dW.shape)\n",
        "    assert(word_vec.shape == dL_dword_vec.shape)\n",
        "    \n",
        "    return dL_dW, dL_dword_vec\n",
        "\n",
        "def backward_propagation(Y, softmax_out, caches):\n",
        "    dL_dZ = softmax_backward(Y, softmax_out)\n",
        "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
        "    \n",
        "    gradients = dict()\n",
        "    gradients['dL_dZ'] = dL_dZ\n",
        "    gradients['dL_dW'] = dL_dW\n",
        "    gradients['dL_dword_vec'] = dL_dword_vec\n",
        "    \n",
        "    return gradients\n",
        "\n",
        "def update_parameters(parameters, caches, gradients, learning_rate):\n",
        "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
        "    inds = caches['inds']\n",
        "    dL_dword_vec = gradients['dL_dword_vec']\n",
        "    m = inds.shape[-1]\n",
        "    \n",
        "    parameters['WRD_EMB'][inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
        "\n",
        "    parameters['W'] -= learning_rate * gradients['dL_dW']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkCBQRBLz2iY"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=False, plot_cost=True):\n",
        "    costs = []\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    if parameters is None:\n",
        "        parameters = initialize_parameters(vocab_size, emb_size)\n",
        "    \n",
        "    begin_time = datetime.now()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_cost = 0\n",
        "        batch_inds = list(range(0, m, batch_size))\n",
        "        #print(batch_inds)\n",
        "        np.random.shuffle(batch_inds)\n",
        "        for i in batch_inds:\n",
        "            X_batch = X[:, i:i+batch_size]\n",
        "            Y_batch = Y[:, i:i+batch_size]\n",
        "\n",
        "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
        "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
        "            update_parameters(parameters, caches, gradients, learning_rate)\n",
        "            cost = cross_entropy(softmax_out, Y_batch)\n",
        "            epoch_cost += np.squeeze(cost)\n",
        "            \n",
        "        costs.append(epoch_cost)\n",
        "        if print_cost and epoch % (epochs // 500) == 0:\n",
        "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
        "        if epoch % (epochs // 100) == 0:\n",
        "            learning_rate *= 0.98\n",
        "    end_time = datetime.now()\n",
        "    print('training time: {}'.format(end_time - begin_time))\n",
        "            \n",
        "    if plot_cost:\n",
        "        plt.plot(np.arange(epochs), costs)\n",
        "        plt.xlabel('# of epochs')\n",
        "        plt.ylabel('cost')\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jUVequfQz9cj",
        "outputId": "faafda2e-aad9-4e9b-885c-d4ecfe2b9f5c"
      },
      "source": [
        "paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 5000, batch_size=128, parameters=None, print_cost=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after epoch 0: 2.552167871982209\n",
            "Cost after epoch 10: 2.551886315779861\n",
            "Cost after epoch 20: 2.551595027916827\n",
            "Cost after epoch 30: 2.5512774669319125\n",
            "Cost after epoch 40: 2.550916894686628\n",
            "Cost after epoch 50: 2.550495223727193\n",
            "Cost after epoch 60: 2.5500022662274797\n",
            "Cost after epoch 70: 2.5494109263800753\n",
            "Cost after epoch 80: 2.548695901468103\n",
            "Cost after epoch 90: 2.547829011944994\n",
            "Cost after epoch 100: 2.546777501776905\n",
            "Cost after epoch 110: 2.545528252213946\n",
            "Cost after epoch 120: 2.5440254104929148\n",
            "Cost after epoch 130: 2.542218224576823\n",
            "Cost after epoch 140: 2.540050891689443\n",
            "Cost after epoch 150: 2.537459695248247\n",
            "Cost after epoch 160: 2.5344329589852954\n",
            "Cost after epoch 170: 2.5308606355859475\n",
            "Cost after epoch 180: 2.5266560372903673\n",
            "Cost after epoch 190: 2.521734221077226\n",
            "Cost after epoch 200: 2.5160101419294496\n",
            "Cost after epoch 210: 2.509531179820701\n",
            "Cost after epoch 220: 2.502155730042855\n",
            "Cost after epoch 230: 2.493832253236392\n",
            "Cost after epoch 240: 2.484555625120194\n",
            "Cost after epoch 250: 2.4743673959700296\n",
            "Cost after epoch 260: 2.4635712118114252\n",
            "Cost after epoch 270: 2.4521711933439336\n",
            "Cost after epoch 280: 2.440360358269329\n",
            "Cost after epoch 290: 2.4284015141337396\n",
            "Cost after epoch 300: 2.416583744898168\n",
            "Cost after epoch 310: 2.4053909445194286\n",
            "Cost after epoch 320: 2.394860870819715\n",
            "Cost after epoch 330: 2.385138693457572\n",
            "Cost after epoch 340: 2.376317902668602\n",
            "Cost after epoch 350: 2.36841918680236\n",
            "Cost after epoch 360: 2.3615238833748764\n",
            "Cost after epoch 370: 2.355420432917934\n",
            "Cost after epoch 380: 2.350002062220878\n",
            "Cost after epoch 390: 2.345168943435148\n",
            "Cost after epoch 400: 2.340819982530916\n",
            "Cost after epoch 410: 2.3369248131085882\n",
            "Cost after epoch 420: 2.3333177420463924\n",
            "Cost after epoch 430: 2.32990529275124\n",
            "Cost after epoch 440: 2.326608139912995\n",
            "Cost after epoch 450: 2.323354202067288\n",
            "Cost after epoch 460: 2.320139549046217\n",
            "Cost after epoch 470: 2.316861289575856\n",
            "Cost after epoch 480: 2.313472368623242\n",
            "Cost after epoch 490: 2.309942736029081\n",
            "Cost after epoch 500: 2.306252995362318\n",
            "Cost after epoch 510: 2.302465212635214\n",
            "Cost after epoch 520: 2.298523312495451\n",
            "Cost after epoch 530: 2.2944286402301652\n",
            "Cost after epoch 540: 2.290197016383405\n",
            "Cost after epoch 550: 2.285849702204386\n",
            "Cost after epoch 560: 2.2814924230719584\n",
            "Cost after epoch 570: 2.277082567031539\n",
            "Cost after epoch 580: 2.272637460982049\n",
            "Cost after epoch 590: 2.268182521346029\n",
            "Cost after epoch 600: 2.263741438319292\n",
            "Cost after epoch 610: 2.2594145214624914\n",
            "Cost after epoch 620: 2.2551475057637025\n",
            "Cost after epoch 630: 2.2509459474431477\n",
            "Cost after epoch 640: 2.24682143513709\n",
            "Cost after epoch 650: 2.242782866094979\n",
            "Cost after epoch 660: 2.238907076246919\n",
            "Cost after epoch 670: 2.235132119393244\n",
            "Cost after epoch 680: 2.2314524215553213\n",
            "Cost after epoch 690: 2.2278690863381203\n",
            "Cost after epoch 700: 2.2243821997954423\n",
            "Cost after epoch 710: 2.2210514981982765\n",
            "Cost after epoch 720: 2.21781867118157\n",
            "Cost after epoch 730: 2.2146755929435615\n",
            "Cost after epoch 740: 2.211621009543246\n",
            "Cost after epoch 750: 2.208653785687564\n",
            "Cost after epoch 760: 2.2058242574366513\n",
            "Cost after epoch 770: 2.2030830397597194\n",
            "Cost after epoch 780: 2.2004240195833673\n",
            "Cost after epoch 790: 2.197847285301735\n",
            "Cost after epoch 800: 2.195353254651222\n",
            "Cost after epoch 810: 2.1929854256181907\n",
            "Cost after epoch 820: 2.1907035550663565\n",
            "Cost after epoch 830: 2.1885038362238434\n",
            "Cost after epoch 840: 2.1863875158894635\n",
            "Cost after epoch 850: 2.184355911105425\n",
            "Cost after epoch 860: 2.182444707748904\n",
            "Cost after epoch 870: 2.1806213090208217\n",
            "Cost after epoch 880: 2.1788827896989362\n",
            "Cost after epoch 890: 2.177230044725341\n",
            "Cost after epoch 900: 2.1756637505822245\n",
            "Cost after epoch 910: 2.1742102836462243\n",
            "Cost after epoch 920: 2.1728434250598405\n",
            "Cost after epoch 930: 2.1715599498387705\n",
            "Cost after epoch 940: 2.1703593937869727\n",
            "Cost after epoch 950: 2.1692409953180913\n",
            "Cost after epoch 960: 2.168221747236766\n",
            "Cost after epoch 970: 2.167281309557045\n",
            "Cost after epoch 980: 2.16641603348648\n",
            "Cost after epoch 990: 2.1656242269862336\n",
            "Cost after epoch 1000: 2.1649040179478405\n",
            "Cost after epoch 1010: 2.164264560201959\n",
            "Cost after epoch 1020: 2.1636912826655044\n",
            "Cost after epoch 1030: 2.163180708760976\n",
            "Cost after epoch 1040: 2.1627306878895234\n",
            "Cost after epoch 1050: 2.1623390435253027\n",
            "Cost after epoch 1060: 2.162009203855602\n",
            "Cost after epoch 1070: 2.1617320331298404\n",
            "Cost after epoch 1080: 2.16150479882524\n",
            "Cost after epoch 1090: 2.16132555313359\n",
            "Cost after epoch 1100: 2.161192412905673\n",
            "Cost after epoch 1110: 2.161104828501415\n",
            "Cost after epoch 1120: 2.1610583856436527\n",
            "Cost after epoch 1130: 2.161051312312124\n",
            "Cost after epoch 1140: 2.1610821503768074\n",
            "Cost after epoch 1150: 2.1611495315943916\n",
            "Cost after epoch 1160: 2.1612500579936764\n",
            "Cost after epoch 1170: 2.161383167768082\n",
            "Cost after epoch 1180: 2.1615479900980477\n",
            "Cost after epoch 1190: 2.161743570595857\n",
            "Cost after epoch 1200: 2.1619690353717935\n",
            "Cost after epoch 1210: 2.1622187782669027\n",
            "Cost after epoch 1220: 2.162495298856971\n",
            "Cost after epoch 1230: 2.1627984566451746\n",
            "Cost after epoch 1240: 2.163127697121843\n",
            "Cost after epoch 1250: 2.1634825246352447\n",
            "Cost after epoch 1260: 2.1638554585226313\n",
            "Cost after epoch 1270: 2.164251440879667\n",
            "Cost after epoch 1280: 2.164670874774583\n",
            "Cost after epoch 1290: 2.165113469395946\n",
            "Cost after epoch 1300: 2.165578967059074\n",
            "Cost after epoch 1310: 2.1660581677383175\n",
            "Cost after epoch 1320: 2.1665579954668694\n",
            "Cost after epoch 1330: 2.167079218678551\n",
            "Cost after epoch 1340: 2.1676216644642112\n",
            "Cost after epoch 1350: 2.1681851635200826\n",
            "Cost after epoch 1360: 2.16875885430208\n",
            "Cost after epoch 1370: 2.1693512802528163\n",
            "Cost after epoch 1380: 2.169963398177066\n",
            "Cost after epoch 1390: 2.1705949969534144\n",
            "Cost after epoch 1400: 2.171245836114377\n",
            "Cost after epoch 1410: 2.17190342681392\n",
            "Cost after epoch 1420: 2.1725776066521014\n",
            "Cost after epoch 1430: 2.17326934391379\n",
            "Cost after epoch 1440: 2.1739782343292684\n",
            "Cost after epoch 1450: 2.174703813516014\n",
            "Cost after epoch 1460: 2.175432068231862\n",
            "Cost after epoch 1470: 2.1761738053919157\n",
            "Cost after epoch 1480: 2.1769298524035148\n",
            "Cost after epoch 1490: 2.1776994955905087\n",
            "Cost after epoch 1500: 2.178481942803062\n",
            "Cost after epoch 1510: 2.1792619276202183\n",
            "Cost after epoch 1520: 2.1800508912177112\n",
            "Cost after epoch 1530: 2.1808494325445635\n",
            "Cost after epoch 1540: 2.1816564973605095\n",
            "Cost after epoch 1550: 2.1824709553518122\n",
            "Cost after epoch 1560: 2.1832767864186504\n",
            "Cost after epoch 1570: 2.184085721219436\n",
            "Cost after epoch 1580: 2.1848981320299377\n",
            "Cost after epoch 1590: 2.185712698074186\n",
            "Cost after epoch 1600: 2.1865280453653946\n",
            "Cost after epoch 1610: 2.1873281030374185\n",
            "Cost after epoch 1620: 2.1881245332988395\n",
            "Cost after epoch 1630: 2.188917557614661\n",
            "Cost after epoch 1640: 2.189705741187503\n",
            "Cost after epoch 1650: 2.1904876299888567\n",
            "Cost after epoch 1660: 2.1912479004130256\n",
            "Cost after epoch 1670: 2.191997769967762\n",
            "Cost after epoch 1680: 2.1927374118835976\n",
            "Cost after epoch 1690: 2.1934654503157933\n",
            "Cost after epoch 1700: 2.194180523201637\n",
            "Cost after epoch 1710: 2.194868807857457\n",
            "Cost after epoch 1720: 2.195540690547733\n",
            "Cost after epoch 1730: 2.196196377762203\n",
            "Cost after epoch 1740: 2.1968346926332782\n",
            "Cost after epoch 1750: 2.1974544960766416\n",
            "Cost after epoch 1760: 2.198044066097664\n",
            "Cost after epoch 1770: 2.198612589487528\n",
            "Cost after epoch 1780: 2.19916034338906\n",
            "Cost after epoch 1790: 2.19968643233098\n",
            "Cost after epoch 1800: 2.2001900116872823\n",
            "Cost after epoch 1810: 2.2006618550390735\n",
            "Cost after epoch 1820: 2.201109639204445\n",
            "Cost after epoch 1830: 2.2015337087750106\n",
            "Cost after epoch 1840: 2.201933477605615\n",
            "Cost after epoch 1850: 2.202308414585796\n",
            "Cost after epoch 1860: 2.2026519782295404\n",
            "Cost after epoch 1870: 2.202970115831884\n",
            "Cost after epoch 1880: 2.2032632120476037\n",
            "Cost after epoch 1890: 2.2035309796505214\n",
            "Cost after epoch 1900: 2.203773184872501\n",
            "Cost after epoch 1910: 2.203985981612722\n",
            "Cost after epoch 1920: 2.204173475465327\n",
            "Cost after epoch 1930: 2.204336055726168\n",
            "Cost after epoch 1940: 2.2044737016841998\n",
            "Cost after epoch 1950: 2.204586441218993\n",
            "Cost after epoch 1960: 2.2046729903871176\n",
            "Cost after epoch 1970: 2.2047356074404707\n",
            "Cost after epoch 1980: 2.204774653054635\n",
            "Cost after epoch 1990: 2.204790329931176\n",
            "Cost after epoch 2000: 2.2047828827091274\n",
            "Cost after epoch 2010: 2.2047533474340595\n",
            "Cost after epoch 2020: 2.2047022230557265\n",
            "Cost after epoch 2030: 2.2046298149317245\n",
            "Cost after epoch 2040: 2.2045365017573104\n",
            "Cost after epoch 2050: 2.2044226965324043\n",
            "Cost after epoch 2060: 2.2042914358747794\n",
            "Cost after epoch 2070: 2.204141622140849\n",
            "Cost after epoch 2080: 2.2039734860518703\n",
            "Cost after epoch 2090: 2.2037875339220836\n",
            "Cost after epoch 2100: 2.2035842982247003\n",
            "Cost after epoch 2110: 2.2033684481838374\n",
            "Cost after epoch 2120: 2.2031375060117666\n",
            "Cost after epoch 2130: 2.2028916151554663\n",
            "Cost after epoch 2140: 2.202631362300799\n",
            "Cost after epoch 2150: 2.2023573520213566\n",
            "Cost after epoch 2160: 2.202075495818887\n",
            "Cost after epoch 2170: 2.201782185637891\n",
            "Cost after epoch 2180: 2.20147747080914\n",
            "Cost after epoch 2190: 2.2011619741476838\n",
            "Cost after epoch 2200: 2.2008363283967936\n",
            "Cost after epoch 2210: 2.2005072983637257\n",
            "Cost after epoch 2220: 2.2001704107069173\n",
            "Cost after epoch 2230: 2.1998256191276795\n",
            "Cost after epoch 2240: 2.199473543398134\n",
            "Cost after epoch 2250: 2.1991148060671866\n",
            "Cost after epoch 2260: 2.198756658284186\n",
            "Cost after epoch 2270: 2.198394025870965\n",
            "Cost after epoch 2280: 2.1980267706851198\n",
            "Cost after epoch 2290: 2.1976554774484947\n",
            "Cost after epoch 2300: 2.197280727790875\n",
            "Cost after epoch 2310: 2.1969099307533977\n",
            "Cost after epoch 2320: 2.196537662581272\n",
            "Cost after epoch 2330: 2.1961637024278344\n",
            "Cost after epoch 2340: 2.1957885767610024\n",
            "Cost after epoch 2350: 2.195412804706861\n",
            "Cost after epoch 2360: 2.1950436745072937\n",
            "Cost after epoch 2370: 2.1946756413075486\n",
            "Cost after epoch 2380: 2.1943084157422055\n",
            "Cost after epoch 2390: 2.19394245230957\n",
            "Cost after epoch 2400: 2.1935781956464377\n",
            "Cost after epoch 2410: 2.1932225896486135\n",
            "Cost after epoch 2420: 2.1928701675080617\n",
            "Cost after epoch 2430: 2.192520589632244\n",
            "Cost after epoch 2440: 2.1921742338170684\n",
            "Cost after epoch 2450: 2.1918314670764594\n",
            "Cost after epoch 2460: 2.1914987203338367\n",
            "Cost after epoch 2470: 2.1911707710558312\n",
            "Cost after epoch 2480: 2.190847250286582\n",
            "Cost after epoch 2490: 2.190528461812683\n",
            "Cost after epoch 2500: 2.1902146989991347\n",
            "Cost after epoch 2510: 2.189911761347\n",
            "Cost after epoch 2520: 2.1896148030606737\n",
            "Cost after epoch 2530: 2.18932344741688\n",
            "Cost after epoch 2540: 2.189037932116542\n",
            "Cost after epoch 2550: 2.188758485666485\n",
            "Cost after epoch 2560: 2.1884902001961333\n",
            "Cost after epoch 2570: 2.188228706837434\n",
            "Cost after epoch 2580: 2.1879736416493456\n",
            "Cost after epoch 2590: 2.1877251870590486\n",
            "Cost after epoch 2600: 2.1874835179757217\n",
            "Cost after epoch 2610: 2.187252976758265\n",
            "Cost after epoch 2620: 2.18702974261981\n",
            "Cost after epoch 2630: 2.186813482480868\n",
            "Cost after epoch 2640: 2.186604335107206\n",
            "Cost after epoch 2650: 2.1864024335133565\n",
            "Cost after epoch 2660: 2.1862113526073\n",
            "Cost after epoch 2670: 2.18602786608887\n",
            "Cost after epoch 2680: 2.1858516864284265\n",
            "Cost after epoch 2690: 2.1856829195479266\n",
            "Cost after epoch 2700: 2.185521667212231\n",
            "Cost after epoch 2710: 2.1853707361737413\n",
            "Cost after epoch 2720: 2.185227520937844\n",
            "Cost after epoch 2730: 2.1850917904187175\n",
            "Cost after epoch 2740: 2.1849636266956924\n",
            "Cost after epoch 2750: 2.184843108951499\n",
            "Cost after epoch 2760: 2.1847322859317986\n",
            "Cost after epoch 2770: 2.1846291836538283\n",
            "Cost after epoch 2780: 2.184533634610928\n",
            "Cost after epoch 2790: 2.184445703740505\n",
            "Cost after epoch 2800: 2.18436545394094\n",
            "Cost after epoch 2810: 2.184294192608975\n",
            "Cost after epoch 2820: 2.1842305762628014\n",
            "Cost after epoch 2830: 2.1841745047020487\n",
            "Cost after epoch 2840: 2.184126030071412\n",
            "Cost after epoch 2850: 2.1840852029332964\n",
            "Cost after epoch 2860: 2.184052609819848\n",
            "Cost after epoch 2870: 2.184027528125837\n",
            "Cost after epoch 2880: 2.1840099258089225\n",
            "Cost after epoch 2890: 2.183999844469688\n",
            "Cost after epoch 2900: 2.1839973242327497\n",
            "Cost after epoch 2910: 2.184002253774983\n",
            "Cost after epoch 2920: 2.1840145153865915\n",
            "Cost after epoch 2930: 2.1840341437607806\n",
            "Cost after epoch 2940: 2.1840611705287483\n",
            "Cost after epoch 2950: 2.184095625679448\n",
            "Cost after epoch 2960: 2.184136725294847\n",
            "Cost after epoch 2970: 2.1841849356973055\n",
            "Cost after epoch 2980: 2.1842403552057847\n",
            "Cost after epoch 2990: 2.1843030048827754\n",
            "Cost after epoch 3000: 2.184372903798494\n",
            "Cost after epoch 3010: 2.1844486233465337\n",
            "Cost after epoch 3020: 2.1845311889705057\n",
            "Cost after epoch 3030: 2.184620758341242\n",
            "Cost after epoch 3040: 2.1847173406784557\n",
            "Cost after epoch 3050: 2.184820942764102\n",
            "Cost after epoch 3060: 2.1849295225972307\n",
            "Cost after epoch 3070: 2.1850446361569373\n",
            "Cost after epoch 3080: 2.1851664954649106\n",
            "Cost after epoch 3090: 2.18529509640051\n",
            "Cost after epoch 3100: 2.1854304319445523\n",
            "Cost after epoch 3110: 2.185569882201267\n",
            "Cost after epoch 3120: 2.185715501517738\n",
            "Cost after epoch 3130: 2.1858675508842853\n",
            "Cost after epoch 3140: 2.186026011494047\n",
            "Cost after epoch 3150: 2.186190861232717\n",
            "Cost after epoch 3160: 2.186358942703585\n",
            "Cost after epoch 3170: 2.1865327728493447\n",
            "Cost after epoch 3180: 2.1867126561250427\n",
            "Cost after epoch 3190: 2.1868985581277656\n",
            "Cost after epoch 3200: 2.18709044084138\n",
            "Cost after epoch 3210: 2.187284655067256\n",
            "Cost after epoch 3220: 2.187484140958543\n",
            "Cost after epoch 3230: 2.187689240991292\n",
            "Cost after epoch 3240: 2.1878999048801453\n",
            "Cost after epoch 3250: 2.1881160785576874\n",
            "Cost after epoch 3260: 2.188333672072627\n",
            "Cost after epoch 3270: 2.1885560058924565\n",
            "Cost after epoch 3280: 2.188783455244952\n",
            "Cost after epoch 3290: 2.189015954390735\n",
            "Cost after epoch 3300: 2.189253433792567\n",
            "Cost after epoch 3310: 2.189491418791402\n",
            "Cost after epoch 3320: 2.18973356403054\n",
            "Cost after epoch 3330: 2.1899802724523534\n",
            "Cost after epoch 3340: 2.1902314640091327\n",
            "Cost after epoch 3350: 2.1904870549920794\n",
            "Cost after epoch 3360: 2.190742246335991\n",
            "Cost after epoch 3370: 2.191000977937122\n",
            "Cost after epoch 3380: 2.191263675660701\n",
            "Cost after epoch 3390: 2.191530246940318\n",
            "Cost after epoch 3400: 2.1918005958231714\n",
            "Cost after epoch 3410: 2.1920696623293985\n",
            "Cost after epoch 3420: 2.1923416205424413\n",
            "Cost after epoch 3430: 2.1926169146860905\n",
            "Cost after epoch 3440: 2.1928954419778806\n",
            "Cost after epoch 3450: 2.193177096636211\n",
            "Cost after epoch 3460: 2.1934566231402006\n",
            "Cost after epoch 3470: 2.193738377347361\n",
            "Cost after epoch 3480: 2.194022817467907\n",
            "Cost after epoch 3490: 2.194309833151933\n",
            "Cost after epoch 3500: 2.1945993115182203\n",
            "Cost after epoch 3510: 2.194885867316306\n",
            "Cost after epoch 3520: 2.195173985321805\n",
            "Cost after epoch 3530: 2.195464133511984\n",
            "Cost after epoch 3540: 2.195756196783158\n",
            "Cost after epoch 3550: 2.1960500580142623\n",
            "Cost after epoch 3560: 2.1963402670019434\n",
            "Cost after epoch 3570: 2.1966313851002264\n",
            "Cost after epoch 3580: 2.1969238859544675\n",
            "Cost after epoch 3590: 2.1972176525114064\n",
            "Cost after epoch 3600: 2.1975125662273083\n",
            "Cost after epoch 3610: 2.197803174287273\n",
            "Cost after epoch 3620: 2.1980940637573\n",
            "Cost after epoch 3630: 2.1983857099547417\n",
            "Cost after epoch 3640: 2.1986779965196037\n",
            "Cost after epoch 3650: 2.1989708061123676\n",
            "Cost after epoch 3660: 2.1992587419664074\n",
            "Cost after epoch 3670: 2.1995463683679937\n",
            "Cost after epoch 3680: 2.199834158414318\n",
            "Cost after epoch 3690: 2.2001219988014804\n",
            "Cost after epoch 3700: 2.2004097757181773\n",
            "Cost after epoch 3710: 2.2006922023916147\n",
            "Cost after epoch 3720: 2.200973775012095\n",
            "Cost after epoch 3730: 2.2012549606880096\n",
            "Cost after epoch 3740: 2.2015356511737894\n",
            "Cost after epoch 3750: 2.201815738133728\n",
            "Cost after epoch 3760: 2.2020900932347023\n",
            "Cost after epoch 3770: 2.2023631030962223\n",
            "Cost after epoch 3780: 2.202635225227587\n",
            "Cost after epoch 3790: 2.2029063580403867\n",
            "Cost after epoch 3800: 2.2031764002091654\n",
            "Cost after epoch 3810: 2.203440424258538\n",
            "Cost after epoch 3820: 2.203702670113767\n",
            "Cost after epoch 3830: 2.203963582277868\n",
            "Cost after epoch 3840: 2.204223067005726\n",
            "Cost after epoch 3850: 2.2044810311006886\n",
            "Cost after epoch 3860: 2.204732784055139\n",
            "Cost after epoch 3870: 2.2049823866733647\n",
            "Cost after epoch 3880: 2.205230267296667\n",
            "Cost after epoch 3890: 2.2054763408142493\n",
            "Cost after epoch 3900: 2.205720522883059\n",
            "Cost after epoch 3910: 2.205958389708584\n",
            "Cost after epoch 3920: 2.2061937954277067\n",
            "Cost after epoch 3930: 2.2064271493630647\n",
            "Cost after epoch 3940: 2.2066583754709996\n",
            "Cost after epoch 3950: 2.2068873986334356\n",
            "Cost after epoch 3960: 2.207110085283162\n",
            "Cost after epoch 3970: 2.207330060257242\n",
            "Cost after epoch 3980: 2.207547711341245\n",
            "Cost after epoch 3990: 2.207762971682087\n",
            "Cost after epoch 4000: 2.2079757754556297\n",
            "Cost after epoch 4010: 2.208182296891066\n",
            "Cost after epoch 4020: 2.208385913709604\n",
            "Cost after epoch 4030: 2.208586990010003\n",
            "Cost after epoch 4040: 2.2087854679981054\n",
            "Cost after epoch 4050: 2.2089812909656557\n",
            "Cost after epoch 4060: 2.209170952962699\n",
            "Cost after epoch 4070: 2.209357571388324\n",
            "Cost after epoch 4080: 2.209541484891808\n",
            "Cost after epoch 4090: 2.2097226444058777\n",
            "Cost after epoch 4100: 2.2099010019680576\n",
            "Cost after epoch 4110: 2.2100733784081985\n",
            "Cost after epoch 4120: 2.210242621915118\n",
            "Cost after epoch 4130: 2.2104090443192916\n",
            "Cost after epoch 4140: 2.2105726047993897\n",
            "Cost after epoch 4150: 2.2107332636276866\n",
            "Cost after epoch 4160: 2.210888170833705\n",
            "Cost after epoch 4170: 2.211039900472749\n",
            "Cost after epoch 4180: 2.2111887365754304\n",
            "Cost after epoch 4190: 2.2113346459824763\n",
            "Cost after epoch 4200: 2.2114775965942006\n",
            "Cost after epoch 4210: 2.21161506606781\n",
            "Cost after epoch 4220: 2.211749352973775\n",
            "Cost after epoch 4230: 2.2118807129349727\n",
            "Cost after epoch 4240: 2.2120091198066447\n",
            "Cost after epoch 4250: 2.21213454845309\n",
            "Cost after epoch 4260: 2.212254799142337\n",
            "Cost after epoch 4270: 2.212371896766879\n",
            "Cost after epoch 4280: 2.2124860682840435\n",
            "Cost after epoch 4290: 2.2125972938846656\n",
            "Cost after epoch 4300: 2.2127055547069068\n",
            "Cost after epoch 4310: 2.2128089656984775\n",
            "Cost after epoch 4320: 2.2129092826203807\n",
            "Cost after epoch 4330: 2.2130067038274572\n",
            "Cost after epoch 4340: 2.213101215161377\n",
            "Cost after epoch 4350: 2.2131928033425226\n",
            "Cost after epoch 4360: 2.213279887655485\n",
            "Cost after epoch 4370: 2.213363961605283\n",
            "Cost after epoch 4380: 2.2134451952949665\n",
            "Cost after epoch 4390: 2.2135235795458668\n",
            "Cost after epoch 4400: 2.2135991059859483\n",
            "Cost after epoch 4410: 2.2136704859490886\n",
            "Cost after epoch 4420: 2.2137389594957915\n",
            "Cost after epoch 4430: 2.2138046690801496\n",
            "Cost after epoch 4440: 2.2138676098597054\n",
            "Cost after epoch 4450: 2.2139277777257056\n",
            "Cost after epoch 4460: 2.213984162255322\n",
            "Cost after epoch 4470: 2.214037760443448\n",
            "Cost after epoch 4480: 2.2140886879139634\n",
            "Cost after epoch 4490: 2.2141369435546\n",
            "Cost after epoch 4500: 2.2141825269149713\n",
            "Cost after epoch 4510: 2.2142246908748984\n",
            "Cost after epoch 4520: 2.2142642009704914\n",
            "Cost after epoch 4530: 2.21430114699437\n",
            "Cost after epoch 4540: 2.2143355310020065\n",
            "Cost after epoch 4550: 2.2143673556414356\n",
            "Cost after epoch 4560: 2.2143961213605388\n",
            "Cost after epoch 4570: 2.2144223747635117\n",
            "Cost after epoch 4580: 2.214446180956643\n",
            "Cost after epoch 4590: 2.214467544647614\n",
            "Cost after epoch 4600: 2.2144864710708108\n",
            "Cost after epoch 4610: 2.2145026920122826\n",
            "Cost after epoch 4620: 2.214516548316025\n",
            "Cost after epoch 4630: 2.2145280816608235\n",
            "Cost after epoch 4640: 2.2145372989391654\n",
            "Cost after epoch 4650: 2.214544207508464\n",
            "Cost after epoch 4660: 2.214548754026259\n",
            "Cost after epoch 4670: 2.2145510871502223\n",
            "Cost after epoch 4680: 2.214551226474902\n",
            "Cost after epoch 4690: 2.214549180657962\n",
            "Cost after epoch 4700: 2.214544958764641\n",
            "Cost after epoch 4710: 2.214538705841418\n",
            "Cost after epoch 4720: 2.214530392124523\n",
            "Cost after epoch 4730: 2.214520016525374\n",
            "Cost after epoch 4740: 2.214507589093474\n",
            "Cost after epoch 4750: 2.2144931202331426\n",
            "Cost after epoch 4760: 2.2144769370675697\n",
            "Cost after epoch 4770: 2.2144588451861624\n",
            "Cost after epoch 4780: 2.2144388242525315\n",
            "Cost after epoch 4790: 2.214416885378753\n",
            "Cost after epoch 4800: 2.214393039983551\n",
            "Cost after epoch 4810: 2.214367781279466\n",
            "Cost after epoch 4820: 2.2143407638400543\n",
            "Cost after epoch 4830: 2.2143119495300043\n",
            "Cost after epoch 4840: 2.2142813502360363\n",
            "Cost after epoch 4850: 2.214248978107854\n",
            "Cost after epoch 4860: 2.2142154769108595\n",
            "Cost after epoch 4870: 2.2141803635623054\n",
            "Cost after epoch 4880: 2.214143583572631\n",
            "Cost after epoch 4890: 2.2141051493536086\n",
            "Cost after epoch 4900: 2.214065073540674\n",
            "Cost after epoch 4910: 2.214024135466941\n",
            "Cost after epoch 4920: 2.2139817273770293\n",
            "Cost after epoch 4930: 2.2139377798524853\n",
            "Cost after epoch 4940: 2.2138923056157656\n",
            "Cost after epoch 4950: 2.2138453175777943\n",
            "Cost after epoch 4960: 2.2137977162834614\n",
            "Cost after epoch 4970: 2.2137487818289796\n",
            "Cost after epoch 4980: 2.213698431260506\n",
            "Cost after epoch 4990: 2.2136466774288803\n",
            "training time: 0:00:01.628121\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxfKEn_l817b",
        "outputId": "587ea688-d64c-4913-9cd1-1223ac0b7098"
      },
      "source": [
        "X_test = np.arange(vocab_size)\n",
        "print(X_test.shape)\n",
        "X_test = np.expand_dims(X_test, axis=0)\n",
        "print(X_test.shape)\n",
        "softmax_test, _ = forward_propagation(X_test, paras)\n",
        "print(softmax_test)\n",
        "top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]\n",
        "print(top_sorted_inds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13,)\n",
            "(1, 13)\n",
            "[[0.13800109 0.0524617  0.0331232  0.13829776 0.07920961 0.05140742\n",
            "  0.13427312 0.01384239 0.13437721 0.0138581  0.00649474 0.03399432\n",
            "  0.13836896]\n",
            " [0.01596826 0.01706903 0.00492235 0.01595091 0.14609616 0.17529387\n",
            "  0.01877578 0.09111718 0.01901427 0.09084067 0.13156048 0.08032384\n",
            "  0.0160552 ]\n",
            " [0.07419133 0.011359   0.03078567 0.07446574 0.03010627 0.19358167\n",
            "  0.10034728 0.03576476 0.09999212 0.03585318 0.02262483 0.16986686\n",
            "  0.07439608]\n",
            " [0.12147247 0.05886362 0.18191928 0.12085532 0.00976401 0.07139496\n",
            "  0.14447256 0.02330484 0.14464643 0.02328538 0.00651482 0.14943693\n",
            "  0.12096703]\n",
            " [0.03910247 0.05367718 0.01978526 0.03922286 0.09954017 0.04322865\n",
            "  0.03348857 0.06085535 0.03365907 0.06118606 0.03265858 0.05955107\n",
            "  0.03931032]\n",
            " [0.00867298 0.24933137 0.10046618 0.00864486 0.03980893 0.05983478\n",
            "  0.02803561 0.06623055 0.02850586 0.06604231 0.13394993 0.01770831\n",
            "  0.00869071]\n",
            " [0.07419986 0.05740191 0.23086895 0.07392824 0.01393987 0.066616\n",
            "  0.07860209 0.25858774 0.07808898 0.25814211 0.0431359  0.18170172\n",
            "  0.07380671]\n",
            " [0.01062926 0.18594833 0.01932966 0.01065561 0.33474859 0.0283277\n",
            "  0.01837351 0.13704498 0.01858475 0.13725411 0.16325854 0.00721809\n",
            "  0.01069265]\n",
            " [0.20843827 0.06901384 0.02860318 0.20831812 0.07487964 0.04945482\n",
            "  0.06418772 0.02616776 0.06392902 0.02619821 0.00322427 0.04062989\n",
            "  0.20840528]\n",
            " [0.01474537 0.04307337 0.12354242 0.01480381 0.05309224 0.05578209\n",
            "  0.12148394 0.14489609 0.12168077 0.14488071 0.42355193 0.04777879\n",
            "  0.01478922]\n",
            " [0.03261913 0.1060322  0.02815252 0.03259245 0.05114148 0.09346425\n",
            "  0.01566503 0.08558597 0.01585224 0.08578676 0.01703097 0.03106783\n",
            "  0.03265817]\n",
            " [0.13882742 0.06024773 0.15348041 0.13919495 0.03002052 0.05782682\n",
            "  0.09565514 0.03489501 0.09530349 0.03496951 0.00553367 0.01389428\n",
            "  0.13911954]\n",
            " [0.12308074 0.03546678 0.04497167 0.12301795 0.03760505 0.05371786\n",
            "  0.14658248 0.02164897 0.14630844 0.02164445 0.01043507 0.16677696\n",
            "  0.12268867]]\n",
            "[[12  8  9 12  0  3  9  1  9  1  1  3 12]\n",
            " [ 0 10 11  0  4 10  0  7  0  7  5 12  0]\n",
            " [11  7  3 11  1  1  3  9  3  9  7  2 11]\n",
            " [ 8  5  6  8  7  2 12  6 12  6  9  6  8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKGKD77683uK",
        "outputId": "db0343cd-6dbc-4542-a7b9-145774b22318"
      },
      "source": [
        "for input_ind in range(vocab_size):\n",
        "    print(input_ind)\n",
        "    input_word = id_to_word[input_ind]\n",
        "    print(input_word)\n",
        "    print(top_sorted_inds[::-1,input_ind])\n",
        "    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n",
        "    print(\"{}'s neighbor words: {}\".format(input_word, output_words))\n",
        "#After the deduction of the costs of investing, beating deduction the stock market deduction is a loser's game.\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "loser's\n",
            "[ 8 11  0 12]\n",
            "loser's's neighbor words: ['game', 'market', \"loser's\", 'a']\n",
            "1\n",
            "costs\n",
            "[ 5  7 10  8]\n",
            "costs's neighbor words: ['beating', 'of', 'investing', 'game']\n",
            "2\n",
            "stock\n",
            "[ 6  3 11  9]\n",
            "stock's neighbor words: ['deduction', 'is', 'market', 'the']\n",
            "3\n",
            "is\n",
            "[ 8 11  0 12]\n",
            "is's neighbor words: ['game', 'market', \"loser's\", 'a']\n",
            "4\n",
            "after\n",
            "[7 1 4 0]\n",
            "after's neighbor words: ['of', 'costs', 'after', \"loser's\"]\n",
            "5\n",
            "beating\n",
            "[ 2  1 10  3]\n",
            "beating's neighbor words: ['stock', 'costs', 'investing', 'is']\n",
            "6\n",
            "deduction\n",
            "[12  3  0  9]\n",
            "deduction's neighbor words: ['a', 'is', \"loser's\", 'the']\n",
            "7\n",
            "of\n",
            "[6 9 7 1]\n",
            "of's neighbor words: ['deduction', 'the', 'of', 'costs']\n",
            "8\n",
            "game\n",
            "[12  3  0  9]\n",
            "game's neighbor words: ['a', 'is', \"loser's\", 'the']\n",
            "9\n",
            "the\n",
            "[6 9 7 1]\n",
            "the's neighbor words: ['deduction', 'the', 'of', 'costs']\n",
            "10\n",
            "investing\n",
            "[9 7 5 1]\n",
            "investing's neighbor words: ['the', 'of', 'beating', 'costs']\n",
            "11\n",
            "market\n",
            "[ 6  2 12  3]\n",
            "market's neighbor words: ['deduction', 'stock', 'a', 'is']\n",
            "12\n",
            "a\n",
            "[ 8 11  0 12]\n",
            "a's neighbor words: ['game', 'market', \"loser's\", 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qcvBSEy9w5F"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}